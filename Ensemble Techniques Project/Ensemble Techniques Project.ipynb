{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNXdftLBHwdq19M132AYnZ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["**<h1>Note : The small functions are created in the Part1 which will be used in the part 2 ML Model building. Please don't consider these small function as they are not nesseccery</h1>**\n","\n"],"metadata":{"id":"lbfPYs4aMnyG"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"iTo7elYolG44","executionInfo":{"status":"error","timestamp":1682695216492,"user_tz":-330,"elapsed":6810,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}},"outputId":"f82cd7a1-7c42-4804-8ff3-43b86205381e"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-64f6694eead3>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","from scipy.stats import zscore\n","import xgboost as xgb\n","from sklearn.metrics import accuracy_score, make_scorer\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","import pickle\n","\n","%matplotlib inline\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Part 1"],"metadata":{"id":"mY7SAzjSle0X"}},{"cell_type":"markdown","source":["##Part 1.1"],"metadata":{"id":"UwZ9PWlXmOsH"}},{"cell_type":"markdown","source":["###Part 1- 1 a. Read ‘TelcomCustomer-Churn_1.csv’ as a DataFrame and assign it to a variable"],"metadata":{"id":"E-eXvJjRlgPf"}},{"cell_type":"code","source":["def loadDataSet(filePath) : \n","  data = pd.read_csv(filePath)\n","  print('Shape of the dataset : ',data.shape)\n","  return data"],"metadata":{"id":"zi9niYOEDrYK","executionInfo":{"status":"aborted","timestamp":1682695216493,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["churn1 = loadDataSet('/content/drive/MyDrive/Colab Notebooks/Ensemble Techniques Project/TelcomCustomer-Churn_1.csv')\n","churn1.head()"],"metadata":{"id":"gxwi0o2plnSe","executionInfo":{"status":"aborted","timestamp":1682695216493,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1- 1 b.Read ‘TelcomCustomer-Churn_2.csv’ as a DataFrame and assign it to a variable"],"metadata":{"id":"Kmq2uhgbmUY_"}},{"cell_type":"code","source":["churn2 = loadDataSet('/content/drive/MyDrive/Colab Notebooks/Ensemble Techniques Project/TelcomCustomer-Churn_2.csv')\n","churn2.head()"],"metadata":{"id":"_E7R_Jw-mWyA","executionInfo":{"status":"aborted","timestamp":1682695216494,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1- 1 c.Merge both the DataFrames on key ‘customerID’ to form a single DataFrame"],"metadata":{"id":"NfOnj9vombsN"}},{"cell_type":"code","source":["df = pd.merge(churn1, churn2, on=\"customerID\")\n","print(df.shape)\n","df.head()"],"metadata":{"id":"t2IrhmHgmdVF","executionInfo":{"status":"aborted","timestamp":1682695216494,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1- 1 d.Verify if all the columns are incorporated in the merged DataFrame by using simple comparisonOperator in Python\n"],"metadata":{"id":"swRffxrfpeA8"}},{"cell_type":"code","source":["def checkIfColumnExist(source,destination): # Function takes the source and destination dataframe and check if all the column from source is present in the destination\n","  for column in source.columns:\n","    if not column in destination :\n","      print('Column {} is not present in the merged data frame'.format(column))\n","      return\n","  \n","  print('All the columns from source are present in the merged data frame')\n"],"metadata":{"id":"A7rE3yPH22d4","executionInfo":{"status":"aborted","timestamp":1682695216494,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkIfColumnExist(churn1,df)\n","checkIfColumnExist(churn2,df)"],"metadata":{"id":"AIUByQuY4jYm","executionInfo":{"status":"aborted","timestamp":1682695216495,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Part 1.2"],"metadata":{"id":"VgQbt-7N254w"}},{"cell_type":"markdown","source":["###Part 1- 2 a Impute missing/unexpected values in the DataFrame"],"metadata":{"id":"jO5BH8zy3B0G"}},{"cell_type":"code","source":["def removeUnwantedColumn(df) : \n","  df = df.drop(['customerID'],axis=1) # droping the customer id since it is not required\n","  return df\n","\n","df = removeUnwantedColumn(df)\n","df.isna().sum()"],"metadata":{"id":"A2o1DL8_3DnZ","executionInfo":{"status":"aborted","timestamp":1682695216495,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above results shows that there is no null or empty value present in the data"],"metadata":{"id":"TK_LfRHfFBoR"}},{"cell_type":"code","source":["for column in df.columns:\n","  print('------------------Value count for {}---------------'.format(column))\n","  print(df[column].value_counts())"],"metadata":{"id":"pz6LKmhg_bAP","executionInfo":{"status":"aborted","timestamp":1682695216495,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the above output we can observ that all the categorical value has the valid data"],"metadata":{"id":"hbn7nU_AHHUI"}},{"cell_type":"markdown","source":["###part 1- 2 b Make sure all the variables with continuous values are of ‘Float’ type\n"],"metadata":{"id":"uTcwJ37wNw_P"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"XVQtlyw4G8k2","executionInfo":{"status":"aborted","timestamp":1682695216496,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. From the above output we can see that the 'TotalCharges' column is a object data which means that the column contains invalid data\n","2. SeniorCitizen is the int data type but it should be changed to categorical variable\n","3. Churn (Target variable) is in the categorical variable covert to interger"],"metadata":{"id":"f_vl2RqeHqMG"}},{"cell_type":"code","source":["def converCategoricalToContinues(df) :\n","  df['SeniorCitizen'] = df['SeniorCitizen'].replace(1, 'Yes')\n","  df['SeniorCitizen'] = df['SeniorCitizen'].replace(0, 'No')\n","\n","  df['SeniorCitizen']\n","\n","  df.TotalCharges[df.TotalCharges.str.isspace()==True]\n","  df['TotalCharges'] = df[['TotalCharges']].apply(lambda x: x.str.strip()).replace('', 0)\n","  df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], downcast=\"float\")\n","\n","  df['Churn'] = df['Churn'].replace('Yes',1)\n","  df['Churn'] = df['Churn'].replace('No',0)\n","\n","  return df\n","\n","df = converCategoricalToContinues(df)\n","df.info()"],"metadata":{"id":"7CF0cmUnH1AH","executionInfo":{"status":"aborted","timestamp":1682695216496,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Successfully converted the 'TotalCharges' column to float data type\n","2. Successfully converted 'SeniorCitizen' column from int to float with proper value replacement\n","3. Successfully converted 'Churn' column from object to int"],"metadata":{"id":"OgFK9_0dNibI"}},{"cell_type":"markdown","source":["###Part 1- 2 c Create a function that will accept a DataFrame as input and return pie-charts for all the appropriate Categorical features. Clearly show percentage distribution in the pie-chart.\n"],"metadata":{"id":"U1pJlVJFN36Y"}},{"cell_type":"code","source":["def createPieChart(df): \n","  \"\"\"\n","  This method is used to plot the pie chart for all the categorical variable\n","  \"\"\"\n","  df1 = df.select_dtypes(include='object')\n","  i=1;\n","  col=3\n","  row=len(df1.columns)\n","  plt.figure(figsize=(20,40))\n","\n","  for column in df1:\n","    x= df1[column].value_counts();\n","    plt.subplot(row, col, i)\n","    plt.title('{} Distribution'.format(column));\n","    plt.pie(x,labels=x.keys(),autopct= (lambda x : '{:.1f}%'.format(x)));\n","    i+=1"],"metadata":{"id":"xrS0rzSYN-qR","executionInfo":{"status":"aborted","timestamp":1682695216496,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["createPieChart(df)"],"metadata":{"id":"admi5NweYYhN","executionInfo":{"status":"aborted","timestamp":1682695216497,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1- 2 d.Share insights for Q2.c "],"metadata":{"id":"3ELoR-AZK61W"}},{"cell_type":"markdown","source":["\n","1. In the given sample the gender and the distribution is split in equally half\n","2. 70% of the customers are Independent customers\n","3. Most of the customers are opting for the optical fiber connection which followed by the DSL\n","4. Cutomer opting for the StreamingTV/Streaming Movies and not opting for the StreamingTV/Streaming Movies is equally discributed (38% to 39% )\n","5. Customers are more convinient with the paperless billing compare the the physical billing\n","6. The Highest opted payment method is Electronic main with 33.6% and followd by mailed check and creadit card(auto). Least opted payment method is Bank Transfer (auto)\n","7. Customer not having internet service will not get the following services<br>\n","    a. Online Security <br>\n","    b. Online Backup <br>\n","    c. Device protection <br>\n","    d. Tech support <br>\n","    e. Streaming Tv <br>\n","    f. Streaming Movies\n","  "],"metadata":{"id":"m3lpD7fOK8en"}},{"cell_type":"markdown","source":["###Part 1 - 2 e.Encode all the appropriate Categorical features with the best suitable approach\n"],"metadata":{"id":"BIYE68WDPYKr"}},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"tPnl0aiaQc9h","executionInfo":{"status":"aborted","timestamp":1682695216497,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataEncoding(df) : \n","  oneHotCols=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n","        'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n","  result= pd.get_dummies(df,columns=oneHotCols)\n","  return result\n","\n","result = dataEncoding(df) \n","result.head(10)"],"metadata":{"id":"UXN5fnX_Q3rS","executionInfo":{"status":"aborted","timestamp":1682695216497,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1- 2 f.Split the data into 80% train and 20% test. \n"],"metadata":{"id":"2xLKvE2wRnqb"}},{"cell_type":"code","source":["def splitData(df,test_size) : \n","  x = df.drop(['Churn'],axis=1)\n","  y = df['Churn']\n","  x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size,random_state=1)\n","  return x_train,x_test,y_train,y_test\n","\n","x_train,x_test,y_train,y_test = splitData(result,0.2)"],"metadata":{"id":"ZpWE-ecsRo-U","executionInfo":{"status":"aborted","timestamp":1682695216498,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1 - 2 g.Normalize/Standardize the data with the best suitable approach"],"metadata":{"id":"WTi_lbiFSRqJ"}},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"2xZhDXY1R9Lw","executionInfo":{"status":"aborted","timestamp":1682695216498,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def standardizingData(x_train,x_test) :\n","  x_train_scaled=x_train.apply(zscore) #Standardizing the training data\n","  x_test_scaled=x_test.apply(zscore) #Standardizing the test data. \n","  return x_train_scaled,x_test_scaled\n","\n","x_train_scaled,x_test_scaled = standardizingData(x_train,x_test)"],"metadata":{"id":"7sRZyDSSXvga","executionInfo":{"status":"aborted","timestamp":1682695216498,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Standardizing the train and test data seperatly since the st and mean changes after the split"],"metadata":{"id":"QcrHzgvtticg"}},{"cell_type":"markdown","source":["##Part 1.3"],"metadata":{"id":"AhZPxJkuvTdv"}},{"cell_type":"markdown","source":["###Part 1 - 3 a. Train a model using XGBoost. Also print best performing parameters along with train and test performance"],"metadata":{"id":"Rt2kHbH3vbWH"}},{"cell_type":"code","source":["param_grid = {\n","    'learning_rate': [0.1, 0.01],\n","    'max_depth': [3, 5, 7],\n","    'n_estimators': [50, 100, 200]\n","}\n","\n","# Initialize XGBoost model\n","xgb_model = xgb.XGBClassifier()\n","\n","# Define scoring metric\n","scorer = make_scorer(accuracy_score)\n","\n","# Perform grid search with cross-validation\n","grid = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring=scorer, cv=5)\n","grid.fit(x_train_scaled, y_train)\n","\n","# Print best performing parameters\n","print(\"Best performing parameters:\", grid.best_params_)\n","\n","# Predict on training and test set with best performing model\n","y_train_pred = grid.predict(x_train_scaled)\n","y_test_pred = grid.predict(x_test_scaled)\n","\n","# Evaluate model performance on training and test set\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(\"Train accuracy: %.2f%%\" % (train_accuracy * 100))\n","print(\"Test accuracy: %.2f%%\" % (test_accuracy * 100))"],"metadata":{"id":"4qEvWlUnvcsG","executionInfo":{"status":"aborted","timestamp":1682695216499,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Part 1 - 3 b Improve performance of the XGBoost as much as possible. Also print best performing parameters along with train and test performance."],"metadata":{"id":"gpU0lvUl8REh"}},{"cell_type":"code","source":["param_grid = {\n","    'learning_rate': [0.01, 0.1, 0.5],\n","    'max_depth': [3, 5, 7],\n","    'n_estimators': [50, 100, 200],\n","    'subsample': [0.5, 0.7, 1.0],\n","    'colsample_bytree': [0.5, 0.7, 1.0],\n","    'reg_alpha': [0, 0.001, 0.01],\n","    'reg_lambda': [0, 0.001, 0.01]\n","}\n","\n","xgb_model = xgb.XGBClassifier()\n","scorer = make_scorer(accuracy_score)\n","\n","# Perform grid search with cross-validation\n","grid = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring=scorer, cv=5)\n","grid.fit(x_train_scaled, y_train)\n","\n","# Print best performing parameters\n","print(\"Best performing parameters:\", grid.best_params_)\n","\n","# Predict on training and test set with best performing model\n","y_train_pred = grid.predict(x_train_scaled)\n","y_test_pred = grid.predict(x_test_scaled)\n","\n","# Evaluate model performance on training and test set\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(\"Train accuracy: %.2f%%\" % (train_accuracy * 100))\n","print(\"Test accuracy: %.2f%%\" % (test_accuracy * 100))"],"metadata":{"id":"s_BZzKGm6Sws","executionInfo":{"status":"aborted","timestamp":1682695216499,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Part 2"],"metadata":{"id":"ERpabR3x87ZS"}},{"cell_type":"markdown","source":["###Part 2 - Build a machine learning workflow that will run autonomously with the csv file and return best performing model"],"metadata":{"id":"zfdrY7uu-Ngo"}},{"cell_type":"code","source":["def findTheBestPerformingModel(filePath):\n","  \"\"\"\n","  This function will take the CSV file as the input and return the best performing\n","  Model. This function will print the nesseccery logs and and write the pickel file \n","  for the best performing model\n","  \"\"\"\n","  sample_data = loadDataSet(filePath)\n","  \n","  print('--------------------------------------------------------------------')\n","  print('Sample Data : ',sample_data.columns)\n","  \n","  # Removing the unwanted data\n","  sample_data = removeUnwantedColumn(sample_data)\n","\n","  #Spliting the data into X and Y and also spliting into train and test data\n","  X_train,X_test,y_train,y_test = splitData(result,0.2)  \n","\n","  print('--------------------------------------------------------------------')\n","  print(\"Independent Variables : \",X_train.columns)\n","  print('--------------------------------------------------------------------')\n","  print(\"Target Variables : \",y_train.name)\n","\n","  preprocessing_pipeline = Pipeline([\n","      ('standard_scaler', StandardScaler())\n","  ])\n","\n","  rf_param_grid = {\n","      'rf__n_estimators': [50, 100, 200],\n","      'rf__max_depth': [3, 5, 7],\n","      'rf__min_samples_split': [2, 4, 6]\n","  }\n","\n","  gb_param_grid = {\n","      'gb__n_estimators': [50, 100, 200],\n","      'gb__max_depth': [3, 5, 7],\n","      'gb__min_samples_split': [2, 4, 6],\n","      'gb__learning_rate': [0.01, 0.1, 0.5]\n","  }\n","\n","  xgb_param_grid = {\n","      'xgb__n_estimators': [50, 100, 200],\n","      'xgb__max_depth': [3, 5, 7],\n","      'xgb__learning_rate': [0.01, 0.1, 0.5],\n","      'xgb__subsample': [0.5, 0.7, 1.0],\n","      'xgb__colsample_bytree': [0.5, 0.7, 1.0],\n","      'xgb__reg_alpha': [0, 0.001, 0.01],\n","      'xgb__reg_lambda': [0, 0.001, 0.01]\n","  }\n","\n","  models = [\n","      (\"rf\", RandomForestClassifier()),\n","      (\"gb\", GradientBoostingClassifier()),\n","      (\"xgb\", XGBClassifier())\n","  ]\n","\n","  best_model = None\n","  best_model_name = ''\n","  best_score = 0\n","\n","  for model_name, model in models:\n","\n","      print('--------------------------------------------------------------------')\n","      print('Building model : ', model_name)\n","      # Define full pipeline with preprocessing and model\n","      full_pipeline = Pipeline([\n","          ('preprocessing', preprocessing_pipeline),\n","          (model_name, model)\n","      ])\n","\n","      # Define parameter grid for current model\n","      if model_name == \"rf\":\n","          param_grid = rf_param_grid\n","      elif model_name == \"gb\":\n","          param_grid = gb_param_grid\n","      elif model_name == \"xgb\":\n","          param_grid = xgb_param_grid\n","\n","      grid = GridSearchCV(full_pipeline, param_grid, cv=5, n_jobs=-1)\n","      grid.fit(X_train, y_train)\n","\n","      # Print best performing parameters\n","      print(\"Best performing parameters for %s: \" % model_name)\n","      print(grid.best_params_)\n","\n","      # Evaluate performance on test set\n","      y_pred = grid.predict(X_test)\n","      score = accuracy_score(y_test, y_pred)\n","      print(\"Accuracy score on test set for \\'{}\\': {:.2f}%\" .format (model_name, score*100))\n","\n","      # Update best model if current model has higher score\n","      if score > best_score:\n","        best_model = model\n","        best_model_name = model_name\n","\n","      \n","  # Open a file for writing in binary mode\n","  pickel_file_name = 'model.pkl'\n","  with open(pickel_file_name, 'wb') as f:\n","  # Serialize the data and write it to the file\n","    pickle.dump(best_model, f)\n","\n","  print('--------------------------------------------------------------------')\n","  print('The Best model is : ',best_model_name)  \n","  print('The pickel file is stored as : ',pickel_file_name)\n","  print('--------------------------------------------------------------------')\n","\n","  return best_model;"],"metadata":{"id":"ZbL0zIV7KMv5","executionInfo":{"status":"aborted","timestamp":1682695216499,"user_tz":-330,"elapsed":14,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filePath = \"/content/drive/MyDrive/Colab Notebooks/Ensemble Techniques Project/TelcomCustomer-Churn_2.csv\";\n","findTheBestPerformingModel(filePath)"],"metadata":{"id":"kTotfl5OKRLC","executionInfo":{"status":"aborted","timestamp":1682695216500,"user_tz":-330,"elapsed":15,"user":{"displayName":"Manjunath R Gowda","userId":"18175269056236750568"}}},"execution_count":null,"outputs":[]}]}